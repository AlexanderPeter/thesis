{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate, prepare and split datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 19\n",
    "split_ratio_1 = 0.2  # (validation+test) / (train+validation+test)\n",
    "split_ratio_2 = 0.5  # (test) / (validation+test)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# NOTE: More quality issues are explained in: https://github.com/Digital-Dermatology/SelfClean-Revised-Benchmarks\n",
    "\n",
    "csv_paths = [\"../datasets/PAD-UFES-20/metadata.csv\"]\n",
    "images_root_paths = [\"../datasets/PAD-UFES-20/images/\"]\n",
    "target_column = \"diagnostic\"\n",
    "file_column = \"img_id\"\n",
    "group_columns = [\"patient_id\"]  # , \"lesion_id\"\n",
    "\n",
    "csv_paths = [\"../datasets/ddi-diverse-dermatology-images/ddi_metadata.csv\"]\n",
    "images_root_paths = [\"../datasets/ddi-diverse-dermatology-images/\"]\n",
    "target_column = \"malignant\"  # \"disease\"\n",
    "file_column = \"DDI_file\"\n",
    "group_columns = []\n",
    "\n",
    "csv_paths = [\n",
    "    \"../datasets/HAM10000/HAM10000_metadata\",\n",
    "    \"../datasets/HAM10000/ISIC2018_Task3_Test_GroundTruth.csv\",\n",
    "]  # Given test dataset\n",
    "images_root_paths = [\"../datasets/HAM10000/images/\"]\n",
    "target_column = \"dx\"\n",
    "file_column = \"image_id\"\n",
    "group_columns = [\"lesion_id\"]\n",
    "# NOTE: The image 'ISIC_0035068' (known as the 'easter egg') is corrupted and was therefore excluded manually from the test dataset!\n",
    "\n",
    "csv_paths = [\"../datasets/fitzpatrick17k/fitzpatrick17k.csv\"]\n",
    "images_root_paths = [\"../datasets/fitzpatrick17k/images/\"]\n",
    "target_column = \"three_partition_label\"  # \"nine_partition_label\"\n",
    "file_column = \"md5hash\"  # \"url\"\n",
    "group_columns = []\n",
    "\n",
    "csv_paths = []\n",
    "images_root_paths = [\"../datasets/PlantDataset/\"]\n",
    "target_column = None\n",
    "file_column = None\n",
    "group_columns = []\n",
    "\n",
    "csv_paths = []\n",
    "images_root_paths = [\n",
    "    \"../datasets/plantdoc-dataset/train/\",\n",
    "    \"../datasets/plantdoc-dataset/test/\",\n",
    "]\n",
    "target_column = None\n",
    "file_column = None\n",
    "group_columns = []\n",
    "# The class \"Tomato two spotted spider mites leaf\" only appears in the train, but not in the test set and was removed manually (2 images)\n",
    "# The following files are originally included in the train and test set and were moved manually to the duplicates subdirectory:\n",
    "# ../datasets/plantdoc-dataset/train/Corn Gray leaf spot/2013Corn_GrayLeafSpot_0815_0003.JPG.jpg\n",
    "# ../datasets/plantdoc-dataset/test/Corn leaf blight/2013Corn_GrayLeafSpot_0815_0003.JPG.jpg\n",
    "# ../datasets/plantdoc-dataset/train/Potato leaf early blight/early-blight-or-target-spot-alternaria-solani-lesions-on-a-tomato-by9j8r.jpg\n",
    "# ../datasets/plantdoc-dataset/train/Tomato Early blight leaf/early-blight-or-target-spot-alternaria-solani-lesions-on-a-tomato-BY9J8R.jpg\n",
    "\n",
    "csv_paths = [\"../datasets/cassava-leaf-disease-classification/train.csv\"]\n",
    "images_root_paths = [\"../datasets/cassava-leaf-disease-classification/train_images/\"]\n",
    "target_column = \"label\"\n",
    "file_column = \"image_id\"\n",
    "group_columns = []\n",
    "\n",
    "csv_paths = []\n",
    "images_root_paths = [\"../datasets/PlantVillage-Dataset/raw/color/\"]\n",
    "target_column = None\n",
    "file_column = None\n",
    "group_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_column is None:\n",
    "    target_column = \".\"\n",
    "    # Use directory structure instead of target column\n",
    "\n",
    "if file_column is None:\n",
    "    file_column = \"filepath\"\n",
    "    # Use default value\n",
    "\n",
    "if 0 == len(csv_paths):\n",
    "    csv_paths = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_duplicates(src_path, duplicates_path):\n",
    "    dst_dir = os.path.join(duplicates_path, os.path.basename(os.path.dirname(src_path)))\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "    dst_path = os.path.join(dst_dir, os.path.basename(src_path))\n",
    "    shutil.move(src_path, dst_path)\n",
    "    return dst_path\n",
    "\n",
    "\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/train/Corn Gray leaf spot/2013Corn_GrayLeafSpot_0815_0003.JPG.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/train/duplicates\",\n",
    "# )\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/test/Corn leaf blight/2013Corn_GrayLeafSpot_0815_0003.JPG.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/test/duplicates\",\n",
    "# )\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/train/Potato leaf early blight/early-blight-or-target-spot-alternaria-solani-lesions-on-a-tomato-by9j8r.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/train/duplicates\",\n",
    "# )\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/train/Tomato Early blight leaf/early-blight-or-target-spot-alternaria-solani-lesions-on-a-tomato-BY9J8R.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/train/duplicates\",\n",
    "# )\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/train/Potato leaf early blight/potato-blight-phytophora-infestans-close-up-of-infected-leaf-top-surface-a60hxg.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/train/duplicates\",\n",
    "# )\n",
    "# move_duplicates(\n",
    "#     \"../datasets/plantdoc-dataset/train/Potato leaf late blight/potato-blight-phytophora-infestans-close-up-of-infected-leaf-top-surface-A60HXG.jpg\",\n",
    "#     \"../datasets/plantdoc-dataset/train/duplicates\",\n",
    "# )\n",
    "\n",
    "# NOTE: In some cases the file hash is not enough to check...\n",
    "# hashlib.md5(open(\"../datasets/plantdoc-dataset/train/Tomato Early blight leaf/earlyblightpotato.jpg\", \"rb\").read()).hexdigest() # d2c45e81c5de5a2f731829ed491e8df5\n",
    "# hashlib.md5(open(\"../datasets/plantdoc-dataset/test/Potato leaf early blight/earlyblightpotato.jpg\", \"rb\").read()).hexdigest() # be11519577cb929d21a68170b2ec88f1\n",
    "# hashlib.md5(Image.open('../datasets/plantdoc-dataset/test/Potato leaf early blight/earlyblightpotato.jpg', \"r\").tobytes()).hexdigest() # 8fec6255afb5003f62de6989e9b40721\n",
    "# hashlib.md5(Image.open('../datasets/plantdoc-dataset/train/Tomato Early blight leaf/earlyblightpotato.jpg', \"r\").tobytes()).hexdigest() # 8fec6255afb5003f62de6989e9b40721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking ../datasets/plantdoc-dataset/train/\n",
      "Found 0 files\n",
      "Found 82 files\n",
      "Found 79 files\n",
      "Found 83 files\n",
      "Found 53 files\n",
      "Found 62 files\n",
      "Found 105 files\n",
      "Found 47 files\n",
      "Found 61 files\n",
      "Found 179 files\n",
      "Found 106 files\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Blueberry leaf\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Corn Gray leaf spot\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Potato leaf early blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Potato leaf late blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Tomato Early blight leaf\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Tomato leaf late blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Tomato leaf yellow virus\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Tomato mold leaf\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\train\\duplicates\\Tomato Septoria leaf spot\n",
      "Found 57 files\n",
      "Found 56 files\n",
      "Found 103 files\n",
      "Found 102 files\n",
      "Found 94 files\n",
      "Found 112 files\n",
      "Found 57 files\n",
      "Found 124 files\n",
      "Found 88 files\n",
      "Found 76 files\n",
      "Found 55 files\n",
      "Found 101 files\n",
      "Found 99 files\n",
      "Found 44 files\n",
      "Found 68 files\n",
      "Found 84 files\n",
      "Found 136 files\n",
      "Checking ../datasets/plantdoc-dataset/test/\n",
      "Found 0 files\n",
      "Found 9 files\n",
      "Found 10 files\n",
      "Found 10 files\n",
      "Found 8 files\n",
      "Found 9 files\n",
      "Found 10 files\n",
      "Found 10 files\n",
      "Found 3 files\n",
      "Found 9 files\n",
      "Found 10 files\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Blueberry leaf\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Corn Gray leaf spot\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Corn leaf blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Potato leaf early blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Potato leaf late blight\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Tomato Early blight leaf\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Tomato leaf bacterial spot\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Tomato leaf yellow virus\n",
      "Skip: ..\\datasets\\plantdoc-dataset\\test\\duplicates\\Tomato Septoria leaf spot\n",
      "Found 12 files\n",
      "Found 8 files\n",
      "Found 9 files\n",
      "Found 6 files\n",
      "Found 5 files\n",
      "Found 7 files\n",
      "Found 8 files\n",
      "Found 6 files\n",
      "Found 8 files\n",
      "Found 9 files\n",
      "Found 8 files\n",
      "Found 8 files\n",
      "Found 10 files\n",
      "Found 10 files\n",
      "Found 5 files\n",
      "Found 6 files\n",
      "Found 10 files\n",
      "Found 40 duplicates\n",
      "Total number of files: 2536\n"
     ]
    }
   ],
   "source": [
    "file_hash_dict = {}\n",
    "duplicates = set()\n",
    "skip_extensions = [\".csv\", \".db\"]\n",
    "\n",
    "\n",
    "def check_root_dir(root_dir):\n",
    "    print(f\"Checking {root_dir}\")\n",
    "    duplicates_path = os.path.normpath(os.path.join(root_dir, \"duplicates/\"))\n",
    "    for sub_path, _, filenames in os.walk(os.path.normpath(root_dir)):\n",
    "        sub_path = os.path.normpath(sub_path)\n",
    "        if duplicates_path in sub_path:\n",
    "            print(f\"Skip: {sub_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Found {len(filenames)} files\")\n",
    "        for filename in filenames:\n",
    "            file_extension = os.path.splitext(filename)[1].lower()\n",
    "            if file_extension in skip_extensions:\n",
    "                print(f\"Skip non image file: {filename}\")\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.normpath(os.path.join(sub_path, filename))\n",
    "            # file_hash = os.path.basename(file_path).lower()\n",
    "            file_hash = hashlib.md5(Image.open(file_path, \"r\").tobytes()).hexdigest()\n",
    "            # file_hash = hashlib.md5(open(file_path, \"rb\").read()).hexdigest()\n",
    "            if file_hash in file_hash_dict:\n",
    "                print(\n",
    "                    f\"File with equal hash ({file_hash}) found: {file_hash_dict[file_hash]}, {file_path}\"\n",
    "                )\n",
    "                move_duplicates(file_path, duplicates_path)\n",
    "                file_hash_dict[file_hash] = move_duplicates(\n",
    "                    file_hash_dict[file_hash], duplicates_path\n",
    "                )\n",
    "            else:\n",
    "                file_hash_dict[file_hash] = file_path\n",
    "\n",
    "    for sub_path, _, filenames in os.walk(os.path.normpath(duplicates_path)):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.normpath(os.path.join(sub_path, filename))\n",
    "            duplicates.add(file_path)\n",
    "\n",
    "\n",
    "for root_dir in images_root_paths:\n",
    "    check_root_dir(root_dir)\n",
    "print(f\"Found {len(duplicates)} duplicates\")\n",
    "\n",
    "existing_file_paths = set(file_hash_dict.values()) - duplicates\n",
    "print(f\"Total number of files: {len(existing_file_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['.', 'filepath']\n",
      "Using predefined testset\n",
      "Total number of rows: 2536\n"
     ]
    }
   ],
   "source": [
    "def init_dataframes(csv_file, rootpath):\n",
    "    df_return = None\n",
    "    if csv_file is None:\n",
    "        file_column_values = []\n",
    "        target_column_values = []\n",
    "        target_dirs = [\n",
    "            name\n",
    "            for name in os.listdir(path=rootpath)\n",
    "            if os.path.isdir(os.path.join(rootpath, name))\n",
    "        ]\n",
    "        for target_dir in target_dirs:\n",
    "            image_file_paths = [\n",
    "                os.path.normpath(os.path.join(rootpath, target_dir, name))\n",
    "                for name in os.listdir(path=os.path.join(rootpath, target_dir))\n",
    "                if os.path.isfile(os.path.join(rootpath, target_dir, name))\n",
    "            ]\n",
    "            length_previous = len(file_column_values)\n",
    "            file_column_values.extend(image_file_paths)\n",
    "            target_column_values.extend([target_dir] * len(image_file_paths))\n",
    "        df_return = pd.DataFrame(\n",
    "            list(zip(target_column_values, file_column_values)),\n",
    "            columns=[target_column, file_column],\n",
    "        )\n",
    "    else:\n",
    "        df_return = pd.read_csv(csv_file)\n",
    "        assert target_column in df_return.columns.values\n",
    "        assert file_column in df_return.columns.values\n",
    "    return df_return\n",
    "\n",
    "\n",
    "df_primary = init_dataframes(csv_paths[0], images_root_paths[0])\n",
    "df_secondary = pd.DataFrame(columns=df_primary.columns)\n",
    "print(f\"Columns: {list(df_primary.columns)}\")\n",
    "if 1 < len(images_root_paths) or 1 < len(csv_paths):\n",
    "    print(\"Using predefined testset\")\n",
    "    df_secondary = init_dataframes(csv_paths[-1], images_root_paths[-1])\n",
    "    assert list(df_primary.columns) == list(df_secondary.columns)\n",
    "print(f\"Total number of rows: {len(df_primary) + len(df_secondary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\datasets\\plantdoc-dataset\\train\\Apple leaf\\20130519yellowingappleleaves.jpg\n",
      "Merge by unique filepath\n",
      "0 rows could not be found\n",
      "0 rows could not be found\n"
     ]
    }
   ],
   "source": [
    "# df_primary[\"filepath\"] = df_primary\n",
    "sample = df_primary.iloc[0]\n",
    "sample[file_column]\n",
    "\n",
    "# sample_path = os.path.normpath(os.path.join(images_root_paths[0], sample[target_column], sample[file_column]))\n",
    "# if not os.path.exists(sample_path):\n",
    "print(sample[file_column])\n",
    "\n",
    "\n",
    "def merge_filepaths():\n",
    "    df_file = pd.DataFrame(\n",
    "        {\n",
    "            \"filepath\": list(existing_file_paths) + list(duplicates),\n",
    "            \"included\": np.concatenate(\n",
    "                [np.ones(len(existing_file_paths)), np.zeros(len(duplicates))]\n",
    "            ).astype(bool),\n",
    "        }\n",
    "    )\n",
    "    if (df_file[\"filepath\"] == sample[file_column]).any():\n",
    "        print(\"Merge by unique filepath\")\n",
    "\n",
    "        df_primary_extended = pd.merge(\n",
    "            df_primary, df_file, left_on=file_column, right_on=\"filepath\"\n",
    "        )\n",
    "        print(f\"{len(df_primary) - len(df_primary_extended)} rows could not be found\")\n",
    "\n",
    "        df_secondary_extended = pd.merge(\n",
    "            df_secondary, df_file, left_on=file_column, right_on=\"filepath\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{len(df_secondary) - len(df_secondary_extended)} rows could not be found\"\n",
    "        )\n",
    "        return df_primary_extended, df_secondary_extended\n",
    "\n",
    "    df_file[\"filename\"] = df_file[\"filepath\"].apply(lambda x: os.path.basename(x))\n",
    "    if not df_file[\"filename\"].is_unique:\n",
    "        print(\"A bit tricky case\")\n",
    "        print(df_file[df_file[\"filename\"].duplicated(False)])\n",
    "        return\n",
    "\n",
    "    if (df_file[\"filename\"] == sample[file_column]).any():\n",
    "        print(\"Merge by unique filename\")\n",
    "\n",
    "    elif df_file[\"filename\"].str.contains(sample[file_column]).sum():\n",
    "        print(\"Merge by substring\")\n",
    "        matching_filenames = df_file[\n",
    "            df_file[\"filename\"].str.contains(sample[file_column])\n",
    "        ][\"filename\"].values\n",
    "        if 1 < len(matching_filenames):\n",
    "            print(f\"Too many matches: {matching_filenames}\")\n",
    "            return\n",
    "        start_idx = matching_filenames[0].find(sample[file_column])\n",
    "        prefix = matching_filenames[0][:start_idx]\n",
    "        postfix = matching_filenames[0][start_idx + len(sample[file_column]) :]\n",
    "        df_primary[file_column] = prefix + df_primary[file_column] + postfix\n",
    "        df_secondary[file_column] = prefix + df_secondary[file_column] + postfix\n",
    "    else:\n",
    "        print(\"No merge found!\")\n",
    "        return\n",
    "\n",
    "    df_primary_extended = pd.merge(\n",
    "        df_primary, df_file, left_on=file_column, right_on=\"filename\"\n",
    "    )\n",
    "    print(f\"{len(df_primary) - len(df_primary_extended)} rows could not be found\")\n",
    "\n",
    "    df_secondary_extended = pd.merge(\n",
    "        df_secondary, df_file, left_on=file_column, right_on=\"filename\"\n",
    "    )\n",
    "    print(f\"{len(df_secondary) - len(df_secondary_extended)} rows could not be found\")\n",
    "    return df_primary_extended, df_secondary_extended\n",
    "\n",
    "\n",
    "df_primary_extended, df_secondary_extended = merge_filepaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 2536\n"
     ]
    }
   ],
   "source": [
    "df_primary = df_primary_extended[df_primary_extended[\"included\"]]\n",
    "df_secondary = df_secondary_extended[df_secondary_extended[\"included\"]]\n",
    "print(f\"Total number of rows: {len(df_primary) + len(df_secondary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stratify(groups, verbose=False):\n",
    "    for name, group in groups:\n",
    "        sub_groups = group.groupby(target_column)\n",
    "        for sub_name, sub_group in sub_groups:\n",
    "            if verbose:\n",
    "                print(f\"{name}: {sub_name}\")\n",
    "            if 1 < len(sub_groups):\n",
    "                print(\"Stratify not possible\")\n",
    "                print(group[[*group_columns, target_column]])\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "groups = None\n",
    "groupby_columns = [df_primary.index]\n",
    "if group_columns == []:\n",
    "    groups = df_primary.groupby(groupby_columns)\n",
    "    assert len(groups) == len(df_primary)\n",
    "else:\n",
    "    groupby_columns = group_columns\n",
    "    groups = df_primary.groupby(groupby_columns)\n",
    "    assert len(groups) < len(df_primary)\n",
    "stratify_possible = check_stratify(groups)\n",
    "\n",
    "if stratify_possible:\n",
    "    previous_length = len(groups)\n",
    "    groups = df_primary.groupby([*groupby_columns, target_column])\n",
    "    assert previous_length == len(groups)\n",
    "\n",
    "df_grouped = groups.size().reset_index()\n",
    "df_grouped.index = df_grouped[\"level_0\"]\n",
    "assert len(df_grouped.index) == len(groups.groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 2056\n",
      "Validation: 257\n",
      "Test (without predefined): 0\n"
     ]
    }
   ],
   "source": [
    "def split_sets(set_combined, split_ratio):\n",
    "    if 0 == split_ratio:\n",
    "        return set_combined, set_combined.iloc[:0, :].copy()\n",
    "\n",
    "    stratify_series = None\n",
    "    if stratify_possible:\n",
    "        stratify_series = set_combined[target_column]\n",
    "\n",
    "    return train_test_split(\n",
    "        set_combined, test_size=split_ratio, random_state=seed, stratify=stratify_series\n",
    "    )\n",
    "\n",
    "\n",
    "if 0 < len(df_secondary):\n",
    "    split_ratio_1 = (\n",
    "        split_ratio_1 * (1 - split_ratio_2) / (1 - split_ratio_1 * split_ratio_2)\n",
    "    )\n",
    "    split_ratio_2 = 0\n",
    "\n",
    "df_train, df_valid_test = split_sets(df_grouped, split_ratio_1)\n",
    "df_valid, df_test = split_sets(df_valid_test, split_ratio_2)\n",
    "\n",
    "train_ids = df_train[group_columns].values\n",
    "valid_ids = df_valid[group_columns].values\n",
    "test_ids = df_test[group_columns].values\n",
    "\n",
    "if group_columns == []:\n",
    "    train_ids = df_train.index.values\n",
    "    valid_ids = df_valid.index.values\n",
    "    test_ids = df_test.index.values\n",
    "\n",
    "print(f\"Training: {len(train_ids)}\")\n",
    "print(f\"Validation: {len(valid_ids)}\")\n",
    "print(f\"Test (without predefined): {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == len(np.intersect1d(train_ids, valid_ids))\n",
    "assert 0 == len(np.intersect1d(valid_ids, test_ids))\n",
    "assert 0 == len(np.intersect1d(test_ids, train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_set(values):\n",
    "    group_id = tuple(values)\n",
    "    if 1 == len(group_id):\n",
    "        group_id = group_id[0]\n",
    "\n",
    "    if group_id in train_ids:\n",
    "        return \"train\"\n",
    "    elif group_id in valid_ids:\n",
    "        return \"valid\"\n",
    "    elif group_id in test_ids:\n",
    "        return \"test\"\n",
    "    else:\n",
    "        print(f\"Group_id '{group_id}' cannot be assigned\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_split = df_primary[[target_column, \"filepath\"]].copy()\n",
    "original_columns = df_split.columns\n",
    "df_split.columns = [\"target_code\", \"filepath\"]\n",
    "\n",
    "if group_columns == []:\n",
    "    df_split[\"set\"] = df_primary.index.to_frame().apply(get_set, axis=1)\n",
    "else:\n",
    "    df_split[\"set\"] = df_primary[groupby_columns].apply(get_set, axis=1)\n",
    "\n",
    "if 0 < len(df_secondary):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    df_split_test = df_secondary[original_columns]\n",
    "    df_split_test[\"set\"] = \"test\"\n",
    "    df_split_test.columns = df_split.columns\n",
    "    assert 0 == (df_split[\"set\"] == \"test\").sum()\n",
    "    df_split = pd.concat([df_split, df_split_test])\n",
    "\n",
    "print(set(df_split.index.unique()) ^ set(df_grouped.index.unique()))\n",
    "df_split[df_split[\"set\"].isnull()][\"filepath\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_code\n",
       "Apple Scab Leaf               0.035992\n",
       "Apple leaf                    0.035506\n",
       "Apple rust leaf               0.034047\n",
       "Bell_pepper leaf              0.022860\n",
       "Bell_pepper leaf spot         0.026751\n",
       "Blueberry leaf                0.045233\n",
       "Cherry leaf                   0.020428\n",
       "Corn Gray leaf spot           0.026265\n",
       "Corn leaf blight              0.077335\n",
       "Corn rust leaf                0.045720\n",
       "Peach leaf                    0.044261\n",
       "Potato leaf early blight      0.044261\n",
       "Potato leaf late blight       0.040856\n",
       "Raspberry leaf                0.048152\n",
       "Soyabean leaf                 0.024805\n",
       "Squash Powdery mildew leaf    0.053502\n",
       "Strawberry leaf               0.037938\n",
       "Tomato Early blight leaf      0.033074\n",
       "Tomato Septoria leaf spot     0.058852\n",
       "Tomato leaf                   0.023833\n",
       "Tomato leaf bacterial spot    0.043774\n",
       "Tomato leaf late blight       0.042802\n",
       "Tomato leaf mosaic virus      0.018969\n",
       "Tomato leaf yellow virus      0.029183\n",
       "Tomato mold leaf              0.036479\n",
       "grape leaf                    0.024805\n",
       "grape leaf black rot          0.024319\n",
       "Name: filepath, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_split[df_split[\"set\"] == \"train\"]\n",
    "df_test.groupby(\"target_code\")[\"filepath\"].count() / len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_code\n",
       "Apple Scab Leaf               0.035019\n",
       "Apple leaf                    0.035019\n",
       "Apple rust leaf               0.035019\n",
       "Bell_pepper leaf              0.023346\n",
       "Bell_pepper leaf spot         0.027237\n",
       "Blueberry leaf                0.046693\n",
       "Cherry leaf                   0.019455\n",
       "Corn Gray leaf spot           0.027237\n",
       "Corn leaf blight              0.077821\n",
       "Corn rust leaf                0.046693\n",
       "Peach leaf                    0.046693\n",
       "Potato leaf early blight      0.042802\n",
       "Potato leaf late blight       0.038911\n",
       "Raspberry leaf                0.050584\n",
       "Soyabean leaf                 0.023346\n",
       "Squash Powdery mildew leaf    0.054475\n",
       "Strawberry leaf               0.038911\n",
       "Tomato Early blight leaf      0.031128\n",
       "Tomato Septoria leaf spot     0.058366\n",
       "Tomato leaf                   0.023346\n",
       "Tomato leaf bacterial spot    0.042802\n",
       "Tomato leaf late blight       0.042802\n",
       "Tomato leaf mosaic virus      0.019455\n",
       "Tomato leaf yellow virus      0.031128\n",
       "Tomato mold leaf              0.035019\n",
       "grape leaf                    0.023346\n",
       "grape leaf black rot          0.023346\n",
       "Name: filepath, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = df_split[df_split[\"set\"] == \"valid\"]\n",
    "df_valid.groupby(\"target_code\")[\"filepath\"].count() / len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_code\n",
       "Apple Scab Leaf               0.044843\n",
       "Apple leaf                    0.040359\n",
       "Apple rust leaf               0.044843\n",
       "Bell_pepper leaf              0.035874\n",
       "Bell_pepper leaf spot         0.040359\n",
       "Blueberry leaf                0.044843\n",
       "Cherry leaf                   0.044843\n",
       "Corn Gray leaf spot           0.013453\n",
       "Corn leaf blight              0.040359\n",
       "Corn rust leaf                0.044843\n",
       "Peach leaf                    0.040359\n",
       "Potato leaf early blight      0.026906\n",
       "Potato leaf late blight       0.022422\n",
       "Raspberry leaf                0.031390\n",
       "Soyabean leaf                 0.035874\n",
       "Squash Powdery mildew leaf    0.026906\n",
       "Strawberry leaf               0.035874\n",
       "Tomato Early blight leaf      0.040359\n",
       "Tomato Septoria leaf spot     0.044843\n",
       "Tomato leaf                   0.035874\n",
       "Tomato leaf bacterial spot    0.035874\n",
       "Tomato leaf late blight       0.044843\n",
       "Tomato leaf mosaic virus      0.044843\n",
       "Tomato leaf yellow virus      0.022422\n",
       "Tomato mold leaf              0.026906\n",
       "grape leaf                    0.053812\n",
       "grape leaf black rot          0.035874\n",
       "Name: filepath, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_split[df_split[\"set\"] == \"test\"]\n",
    "df_test.groupby(\"target_code\")[\"filepath\"].count() / len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_root_path = images_root_paths[0]\n",
    "if 1 < len(images_root_paths):\n",
    "    images_root_path = os.path.commonpath(images_root_paths)\n",
    "df_split.to_csv(os.path.join(images_root_path, \"split.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_with_case_deviations = df_merged[df_merged[file_column].str.lower().duplicated(False)][file_column]\n",
    "# if 0 < len(files_with_case_deviations):\n",
    "#     print(files_with_case_deviations)\n",
    "#     assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_wrapper(src, dst):\n",
    "#     if os.path.exists(dst):\n",
    "#         print(f\"Cannot copy from {src} to {dst}\")\n",
    "#         return\n",
    "#     return shutil.copy(src, dst)\n",
    "\n",
    "# if type(images_root_paths) is tuple:\n",
    "#     common_path = os.path.join(os.path.commonpath(images_root_paths), \"all/\")\n",
    "#     os.makedirs(common_path, exist_ok=False)\n",
    "#     shutil.copytree(images_root_paths[0], common_path, dirs_exist_ok=True, copy_function = copy_wrapper)\n",
    "#     shutil.copytree(images_root_paths[1], common_path, dirs_exist_ok=True, copy_function = copy_wrapper)\n",
    "#     images_root_paths = common_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not df_merged[file_column].is_unique:\n",
    "#     assert df_merged[[target_column,file_column]].apply(lambda x: os.path.join(*x), axis=1).is_unique\n",
    "#     df_primary[file_column] = df_primary[[target_column,file_column]].apply(lambda x: os.path.join(\"..\", *x), axis=1)\n",
    "#     assert df_primary[file_column].is_unique\n",
    "#     df_secondary[file_column] = df_secondary[[target_column,file_column]].apply(lambda x: os.path.join(\"..\", *x), axis=1)\n",
    "#     assert df_secondary[file_column].is_unique\n",
    "#     df_merged = pd.concat([df_primary, df_secondary])\n",
    "#     assert df_merged[file_column].is_unique\n",
    "\n",
    "# df_merged[[file_column, target_column]].groupby(target_column).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_existing_file(df):\n",
    "#     for image_path in df[df[target_column] == value][file_column]:\n",
    "#         image_name = os.path.basename(image_path)\n",
    "\n",
    "#         if not image_name in file_name_list:\n",
    "#             if not image_name in file_name_list:\n",
    "#             if image_name = image_name.replace(\"?\", \"\") # windows characters\n",
    "\n",
    "\n",
    "#         file_extension = os.path.splitext(image_name)[1]\n",
    "\n",
    "#         if file_extension == \"\":\n",
    "#             found_image_names = [\n",
    "#                 image_file_name\n",
    "#                 for image_file_name in image_file_names\n",
    "#                 if image_file_name.startswith(image_name)\n",
    "#             ]\n",
    "\n",
    "#             if 1 != len(found_image_names):\n",
    "#                 print(\n",
    "#                     f\"Image name '{image_name}' cannot be assigned to existing files: {found_image_names}\"\n",
    "#                 )\n",
    "#             assert 1 == len(found_image_names)\n",
    "#             image_name_new = found_image_names[0]\n",
    "#             df.loc[df[file_column] == image_name, [file_column]] = image_name_new\n",
    "#             image_name = image_name_new\n",
    "\n",
    "# if os.path.exists(source_path):\n",
    "#             else:\n",
    "#                 print(f\"Missing file: {source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def copy_images_to_target_subdirectory(df):\n",
    "#     if csv_paths is None:\n",
    "#         return\n",
    "\n",
    "#     image_file_names = [\n",
    "#         name\n",
    "#         for name in os.listdir(path=images_root_paths)\n",
    "#         if os.path.isfile(os.path.join(images_root_paths, name))\n",
    "#     ]\n",
    "\n",
    "#     for value in df[target_column].unique():\n",
    "#         subdirectory = os.path.join(images_root_paths, target_column, str(value))\n",
    "#         print(f\"Copying to {subdirectory}\")\n",
    "#         os.makedirs(subdirectory, exist_ok=True)\n",
    "#         for image_path in df[df[target_column] == value][file_column]:\n",
    "#             image_name = os.path.basename(image_path)\n",
    "#             source_path = os.path.join(images_root_paths, image_name)\n",
    "#             if not os.path.exists(os.path.join(subdirectory, image_name)):\n",
    "#                 shutil.copy(source_path, f\"{subdirectory}/\")\n",
    "\n",
    "# copy_images_to_target_subdirectory(df_primary)\n",
    "\n",
    "# if df_secondary is not None:\n",
    "#     copy_images_to_target_subdirectory(df_secondary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example_images(title, image_paths):\n",
    "    number_of_columns = len(image_paths)\n",
    "    number_of_rows = len(image_paths[0])\n",
    "    fig, ax = plt.subplots(\n",
    "        number_of_columns,\n",
    "        number_of_rows,\n",
    "        figsize=(3 * number_of_rows, 3 * number_of_columns),\n",
    "        squeeze=False,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    for row in range(len(image_paths)):\n",
    "        for column in range(len(image_paths[0])):\n",
    "            image_path = image_paths[row][column]\n",
    "            pil_im = Image.open(image_path, \"r\")\n",
    "            title = os.path.basename(image_path)\n",
    "            ax[row][column].imshow(pil_im)\n",
    "            ax[row][column].set_title(title)\n",
    "            ax[row][column].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    [\n",
    "        \"../datasets/HAM10000/images/ISIC_0033084.jpg\",\n",
    "        \"../datasets/HAM10000/images/ISIC_0033550.jpg\",\n",
    "        \"../datasets/HAM10000/images/ISIC_0033536.jpg\",\n",
    "    ],\n",
    "]\n",
    "# plot_example_images(\"HAM10000 examples\", image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: grouping is tricky. Examples:\n",
    "# PAT_1064_272_668, PAT_1064_273_980\n",
    "# PAT_759_1538_566, PAT_759_1433_914\n",
    "# PAT_1216_759_365, PAT_1216_759_542\n",
    "\n",
    "# Different lesions should not get mixed up\n",
    "# Sometimes different lesions have the same image\n",
    "\n",
    "# <patient_id>_<lesion_id>_<image_number>.png\n",
    "image_paths = [\n",
    "    [\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1064_273_980.png\",\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1064_272_668.png\",\n",
    "    ],\n",
    "    [\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1288_1003_553.png\",\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1288_1003_969.png\",\n",
    "    ],\n",
    "    [\n",
    "        \"../datasets/PAD-UFES-20/images/duplicates/images/PAT_38_1003_68.png\",\n",
    "        \"../datasets/PAD-UFES-20/images/duplicates/images/PAT_38_1003_226.png\",\n",
    "    ],\n",
    "    [\n",
    "        \"../datasets/PAD-UFES-20/images/duplicates/images/PAT_759_1538_566.png\",\n",
    "        \"../datasets/PAD-UFES-20/images/duplicates/images/PAT_759_1433_914.png\",\n",
    "    ],\n",
    "    [\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1216_759_365.png\",\n",
    "        \"../datasets/PAD-UFES-20/images/PAT_1216_759_542.png\",\n",
    "    ],\n",
    "]\n",
    "# plot_example_images(\"PAD-UFES-20 examples\", image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Only used to download Fitzpatrick17k images from original source, but many links are dead. It is better to download the dataset from the Google Drive.\n",
    "#\n",
    "# if \"url\" in df_primary.columns:\n",
    "#     for image_url in df_primary[\"url\"]:\n",
    "#         file_name = None\n",
    "#         if str(image_url) == \"nan\":\n",
    "#             # print(f\"image_url: {image_url}\")\n",
    "#             continue\n",
    "#         elif str(image_url).startswith(\"https://www.dermaamin.com\"):\n",
    "#             continue\n",
    "#         elif str(image_url).startswith(\"http://atlasdermatologico.com.br/img\"):\n",
    "#             continue\n",
    "\n",
    "#         file_name = os.path.basename(image_url)\n",
    "#         file_name = file_name.replace(\"?\", \"\")\n",
    "#         file_path = os.path.join(images_root_paths, file_name)\n",
    "\n",
    "#         if not os.path.exists(file_path):\n",
    "#             response = requests.get(\n",
    "#                 image_url, stream=True, headers={\"User-Agent\": \"XY\"}\n",
    "#             )\n",
    "#             if not response.ok:\n",
    "#                 print(f\"image_url: {image_url}\")\n",
    "#                 print(response)\n",
    "#                 continue\n",
    "\n",
    "#             with open(file_path, \"wb\") as handle:\n",
    "#                 for block in response.iter_content(1024):\n",
    "#                     if not block:\n",
    "#                         break\n",
    "#                     handle.write(block)\n",
    "# else:\n",
    "#     print(\"Skip download\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "412f64844b7b8af74bdbb265a073648c443bf75ef51a1e949163ab9198702ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
