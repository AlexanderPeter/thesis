{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_root_path = \"../datasets/intermediate-features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(targets, predictions):\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    balanced_accuracy = balanced_accuracy_score(targets, predictions)\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    f1_weighted = f1_score(targets, predictions, average=\"weighted\")\n",
    "    print(f\"F1-weighted: {f1_weighted}\")\n",
    "    f1_micro = f1_score(targets, predictions, average=\"micro\")\n",
    "    print(f\"F1-micro: {f1_micro}\")\n",
    "    f1_macro = f1_score(targets, predictions, average=\"macro\")\n",
    "    print(f\"F1-macro: {f1_macro}\")\n",
    "    # TODO: add to log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = os.listdir(path=csv_root_path)\n",
    "df_comparison = pd.DataFrame()\n",
    "\n",
    "for i, path in enumerate(csv_paths):\n",
    "    df_full = pd.read_csv(os.path.join(csv_root_path, path), index_col=0)\n",
    "    df_comparison[f\"target_{i}\"] = df_full[\"target\"]\n",
    "    df_comparison[f\"set_{i}\"] = df_full[\"set\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_0     False\n",
       "target_1      True\n",
       "target_2      True\n",
       "target_3      True\n",
       "target_4      True\n",
       "target_5      True\n",
       "target_6      True\n",
       "target_7      True\n",
       "target_8      True\n",
       "target_9      True\n",
       "target_10     True\n",
       "target_11     True\n",
       "target_12     True\n",
       "target_13     True\n",
       "target_14     True\n",
       "target_15     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparison[df_comparison.columns[0::2]].T.duplicated()  # verify if split is the same\n",
    "# first row must be False, all other True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_0     False\n",
       "set_1      True\n",
       "set_2      True\n",
       "set_3      True\n",
       "set_4      True\n",
       "set_5      True\n",
       "set_6      True\n",
       "set_7      True\n",
       "set_8      True\n",
       "set_9      True\n",
       "set_10     True\n",
       "set_11     True\n",
       "set_12     True\n",
       "set_13     True\n",
       "set_14     True\n",
       "set_15     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparison[df_comparison.columns[1::2]].T.duplicated()  # verify if split is the same\n",
    "# first row must be False, all other True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(csv_root_path, csv_paths[0])\n",
    "\n",
    "df_full = pd.read_csv(csv_path, index_col=0)\n",
    "df_train = df_full[df_full[\"set\"] == \"train\"]\n",
    "df_valid = df_full[df_full[\"set\"] == \"valid\"]\n",
    "df_test = df_full[df_full[\"set\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features = df_train.loc[:, ~df_train.columns.isin([\"target\", \"set\"])]\n",
    "df_valid_features = df_valid.loc[:, ~df_valid.columns.isin([\"target\", \"set\"])]\n",
    "df_test_features = df_test.loc[:, ~df_test.columns.isin([\"target\", \"set\"])]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(df_train_features)\n",
    "valid_features = scaler.transform(df_valid_features)\n",
    "test_features = scaler.transform(df_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = df_train[\"target\"].to_numpy()\n",
    "valid_targets = df_valid[\"target\"].to_numpy()\n",
    "test_targets = df_test[\"target\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49707602339181284\n",
      "Balanced accuracy: 0.18841269841269842\n",
      "F1-weighted: 0.47049228480976374\n",
      "F1-micro: 0.49707602339181284\n",
      "F1-macro: 0.18185394182561992\n",
      "---\n",
      "Accuracy: 0.4941860465116279\n",
      "Balanced accuracy: 0.19741223202289854\n",
      "F1-weighted: 0.47738929595412566\n",
      "F1-micro: 0.4941860465116279\n",
      "F1-macro: 0.18899783705839945\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "model_lr.fit(train_features, train_targets)\n",
    "\n",
    "valid_pred = model_lr.predict(valid_features)\n",
    "calculate_scores(valid_targets, valid_pred)\n",
    "print(\"---\")\n",
    "test_pred = model_lr.predict(test_features)\n",
    "calculate_scores(test_targets, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(targets, predictions):\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    balanced_accuracy = balanced_accuracy_score(targets, predictions)\n",
    "    print(f\"Balanced accuracy: {balanced_accuracy}\")\n",
    "    f1_weighted = f1_score(targets, predictions, average=\"weighted\")\n",
    "    print(f\"F1-weighted: {f1_weighted}\")\n",
    "    f1_micro = f1_score(targets, predictions, average=\"micro\")\n",
    "    print(f\"F1-micro: {f1_micro}\")\n",
    "    f1_macro = f1_score(targets, predictions, average=\"macro\")\n",
    "    print(f\"F1-macro: {f1_macro}\")\n",
    "    # TODO: add to log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with 15 neighbors: 0.525850771507081\n",
      "Score with 15 neighbors: 0.49388110677755936\n",
      "Score with 15 neighbors: 0.47750100441944543\n",
      "Score with 15 neighbors: 0.4862665435518621\n",
      "Score with 15 neighbors: 0.48095075916434227\n",
      "Score with 15 neighbors: 0.46612882571391984\n",
      "Score with 15 neighbors: 0.4672006102212051\n",
      "Score with 15 neighbors: 0.4672006102212051\n",
      "Score with 15 neighbors: 0.4672006102212051\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "max_neighbors = 10\n",
    "for k in range(1, max_neighbors):\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    model_knn.fit(train_features, train_targets)\n",
    "    valid_pred = model_knn.predict(valid_features)\n",
    "    score = f1_score(df_valid_targets, valid_pred, average=\"weighted\")\n",
    "    scores[k] = score\n",
    "    print(f\"Score with {i} neighbors: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6046511627906976\n",
      "Balanced accuracy: 0.2996058403500427\n",
      "F1-weighted: 0.5548930992384169\n",
      "F1-micro: 0.6046511627906976\n",
      "F1-macro: 0.3106667970652645\n"
     ]
    }
   ],
   "source": [
    "best_k = max(scores, key=scores.get)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "model_knn.fit(train_features, train_targets)\n",
    "test_pred = model_knn.predict(test_features)\n",
    "calculate_scores(test_targets, test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "412f64844b7b8af74bdbb265a073648c443bf75ef51a1e949163ab9198702ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
