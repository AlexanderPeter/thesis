{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import ViTModel\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import pathlib\n",
    "\n",
    "temp = pathlib.PosixPath\n",
    "\n",
    "from ssl_library.src.pkg.embedder import Embedder\n",
    "from ssl_library.src.pkg.wrappers import ViTWrapper, Wrapper\n",
    "# from ssl_library.src.models.encoders.vision_transformer import vit_tiny\n",
    "\n",
    "from local_python.local_utils import print_parameters, load_headless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 19\n",
    "all_checkpoint_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# NOTE: ssl_library is not windows compatible in itself\n",
    "# https://stackoverflow.com/questions/57286486/i-cant-load-my-model-because-i-cant-put-a-posixpath\n",
    "\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_checkpoint_keys(model_dir, n=0, depth=2):\n",
    "    checkpoint = torch.load(model_dir, map_location=torch.device(\"cpu\"))\n",
    "    checkpoint_keys = list(checkpoint.keys())\n",
    "    print(\n",
    "        f\"Key prefixes: {sorted(set('.'.join(x.split('.')[:depth]) for x in checkpoint_keys))}\"\n",
    "    )\n",
    "    # print(f\"Key prefixes: {set(x.split('.')[0] for x in checkpoint_keys)}\")\n",
    "    all_checkpoint_paths.append(model_dir)\n",
    "    if n <= 0:\n",
    "        print(f\"{len(checkpoint_keys)} keys in total.\")\n",
    "        return\n",
    "    print(f\"{len(checkpoint_keys)} keys in total. First {n} keys: \")\n",
    "    for key in checkpoint_keys[:n]:\n",
    "        value = checkpoint[key]\n",
    "        if 100 < len(str(value)):\n",
    "            value = type(value)\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/resnet50/ResNet50-PDDD_raw.pth already exists\n",
      "File ../model_weights/resnet50/ResNet50-PDDD_headless.pth already exists\n",
      "Key prefixes: ['model.0', 'model.1', 'model.4', 'model.5', 'model.6', 'model.7']\n",
      "318 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/resnet50/ResNet50-PDDD_raw.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model_url = \"https://zenodo.org/records/7890438/files/ResNet50-Plant-model-80.pth\"\n",
    "    torch.hub.download_url_to_file(model_url, model_dir, progress=True)\n",
    "\n",
    "raw_path = model_dir\n",
    "model_dir = \"../model_weights/resnet50/ResNet50-PDDD_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_pretrained(\"resnet50_random\")\n",
    "    num_classes_weights = 120\n",
    "    model = resnet50(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes_weights)\n",
    "    print(f\"model.fc.in_features: {model.fc.in_features}\")\n",
    "    checkpoint = torch.load(raw_path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "    print(f\"model.fc.out_features: {model.fc.out_features}\")\n",
    "    # NOTE: The Wrapper from the ssl_library adds a prefic to the dictionary keys (replace_ckp_str=\"model.\")\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model = Wrapper(model=model)\n",
    "    print_parameters(model)  # 23'508'032\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/resnet50/ResNet50-Random_headless.pth already exists\n",
      "Key prefixes: ['model.0', 'model.1', 'model.4', 'model.5', 'model.6', 'model.7']\n",
      "318 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/resnet50/ResNet50-Random_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    # NOTE: uses ResNet50_Weights.IMAGENET1K_V1 instead of default ResNet50_Weights.IMAGENET1K_V2\n",
    "    model = Embedder.load_pretrained(\"resnet50_random\")\n",
    "    model.fc = nn.Sequential()\n",
    "    print_parameters(model)  # 23'508'032\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/resnet50/ResNet50-ImageNet_1k_SL_V1_headless.pth already exists\n",
      "Key prefixes: ['model.0', 'model.1', 'model.4', 'model.5', 'model.6', 'model.7']\n",
      "318 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/resnet50/ResNet50-ImageNet_1k_SL_V1_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_pretrained(\"imagenet\")\n",
    "    model.fc = nn.Sequential()\n",
    "    print_parameters(model)  # 23'508'032\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_raw.pth already exists\n",
      "File ../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_headless.pth already exists\n",
      "Key prefixes: ['model.0', 'model.1', 'model.4', 'model.5', 'model.6', 'model.7']\n",
      "318 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_raw.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model_url = \"https://github.com/vm02-self-supervised-dermatology/self-supervised-models/tree/main/simclr_imagenet/resnet50_imagenet_bs2k_epochs600.pth.tar\"\n",
    "    torch.hub.download_url_to_file(model_url, model_dir, progress=True)\n",
    "\n",
    "raw_path = model_dir\n",
    "model_dir = \"../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    # NOTE: This model is not wrapped in ssl_library!\n",
    "    # Therefore it get wrapped here to use common dictionary keys (replace_ckp_str=\"model.\")\n",
    "    model = Embedder.load_simclr_imagenet(raw_path)\n",
    "    model = Wrapper(model=model)\n",
    "    print_parameters(model)  # 23'508'032\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_raw.pth already exists\n",
      "File ../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_headless.pth already exists\n",
      "Key prefixes: ['model.0', 'model.1', 'model.4', 'model.5', 'model.6', 'model.7']\n",
      "318 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_raw.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model_url = \"https://github.com/vm02-self-supervised-dermatology/self-supervised-models/raw/main/simclr/checkpoint-epoch100.pth\"\n",
    "    torch.hub.download_url_to_file(model_url, model_dir, progress=True)\n",
    "\n",
    "raw_path = model_dir\n",
    "model_dir = \"../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    # NOTE: This model is not wrapped in ssl_library!\n",
    "    # Therefore it get wrapped here to use common dictionary keys (replace_ckp_str=\"model.\")\n",
    "    model = Embedder.load_simclr(raw_path)\n",
    "    model = Wrapper(model=model)\n",
    "    print_parameters(model)  # 23'508'032\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v2/ViT_T16-ImageNet_AugReg_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-ImageNet_AugReg_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = timm.create_model(\"vit_tiny_patch16_224\", pretrained=True)\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5524416\n",
    "    model = ViTWrapper(model)\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v1/ViT_T16-ImageNet_1k_SL_WinKawaks_headless.pth already exists\n",
      "Key prefixes: ['model.embeddings', 'model.encoder', 'model.layernorm', 'model.pooler']\n",
      "200 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/vit_t16_v1/ViT_T16-ImageNet_1k_SL_WinKawaks_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_pretrained(\"imagenet_vit_tiny\")\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'561'472\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)\n",
    "# TODO: teacher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_raw.pth already exists\n",
      "File ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n",
      "File ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_teacher_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_raw.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model_url = \"https://github.com/vm02-self-supervised-dermatology/self-supervised-models/raw/main/imagenet_dino/checkpoint-epoch100.pth\"\n",
    "    torch.hub.download_url_to_file(model_url, model_dir, progress=True)\n",
    "\n",
    "raw_path = model_dir\n",
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path)\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)\n",
    "\n",
    "model_dir = (\n",
    "    \"../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_teacher_headless.pth\"\n",
    ")\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path, model_key=\"teacher\")\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n",
      "File ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_teacher_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"../model_weights/vit_t16_v2/model_best 1.pth\"\n",
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path)\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)\n",
    "\n",
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_teacher_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path, model_key=\"teacher\")\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v2/ViT_T16-Random_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Random_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_pretrained(\"vit_tiny_random\")\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_raw.pth already exists\n",
      "File ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n",
      "File ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_teacher_headless.pth already exists\n",
      "Key prefixes: ['model.blocks', 'model.cls_token', 'model.norm', 'model.patch_embed', 'model.pos_embed']\n",
      "150 keys in total.\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_raw.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model_url = \"https://github.com/vm02-self-supervised-dermatology/self-supervised-models/raw/main/dino/checkpoint-epoch100.pth\"\n",
    "    torch.hub.download_url_to_file(model_url, model_dir, progress=True)\n",
    "\n",
    "raw_path = model_dir\n",
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path)\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)\n",
    "\n",
    "model_dir = \"../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_teacher_headless.pth\"\n",
    "if os.path.exists(model_dir):\n",
    "    print(f\"File {model_dir} already exists\")\n",
    "else:\n",
    "    model = Embedder.load_dino(raw_path, model_key=\"teacher\")\n",
    "    model.head = nn.Sequential()\n",
    "    print_parameters(model)  # 5'524'416\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "    print(f\"File {model_dir} saved\")\n",
    "print_checkpoint_keys(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-PDDD_headless.pth\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-PDDD_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-Random_headless.pth\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-Random_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-ImageNet_1k_SL_V1_headless.pth\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-ImageNet_1k_SL_V1_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_headless.pth\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-ImageNet_1k_SSL_SimCLR_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_headless.pth\n",
      "Loading model with architecture 'resnet50' from ../model_weights/resnet50/ResNet50-Derma_SSL_SimCLR_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_AugReg_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_AugReg_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v1' from ../model_weights/vit_t16_v1/ViT_T16-ImageNet_1k_SL_WinKawaks_headless.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with architecture 'vit_t16_v1' from ../model_weights/vit_t16_v1/ViT_T16-ImageNet_1k_SL_WinKawaks_headless.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_teacher_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_teacher_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_teacher_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_teacher_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Random_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Random_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_headless.pth\n",
      "Ignoring prefix 'model.'\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_teacher_headless.pth\n",
      "Loading model with architecture 'vit_t16_v2' from ../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_teacher_headless.pth\n",
      "Ignoring prefix 'model.'\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_path in all_checkpoint_paths:\n",
    "    _ = load_headless_model(checkpoint_path, wrapped=True, ignore_key_prefix=False)\n",
    "    _ = load_headless_model(checkpoint_path, wrapped=False, ignore_key_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 200\n",
      "Total parameters: 5561472\n",
      "Number of entries: 150\n",
      "Total parameters: 5524416\n"
     ]
    }
   ],
   "source": [
    "model = Embedder.load_pretrained(\"imagenet_vit_tiny\")\n",
    "model.head = nn.Sequential()\n",
    "print_parameters(model)\n",
    "params1 = {}\n",
    "for name, parameter in model.named_parameters():\n",
    "    param_count = parameter.numel()\n",
    "    params1[name] = param_count\n",
    "\n",
    "model = Embedder.load_pretrained(\"vit_tiny_random\")\n",
    "model.head = nn.Sequential()\n",
    "print_parameters(model)\n",
    "params2 = {}\n",
    "for name, parameter in model.named_parameters():\n",
    "    param_count = parameter.numel()\n",
    "    params2[name] = param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len1: 200, len2: 150\n"
     ]
    }
   ],
   "source": [
    "keys1 = set(params1.keys())\n",
    "sizedict1 = {}\n",
    "for k, v in params1.items():\n",
    "    sizedict1[v] = sizedict1.get(v, set()) | set([k])\n",
    "\n",
    "keys2 = set(params2.keys())\n",
    "sizedict2 = {}\n",
    "for k, v in params2.items():\n",
    "    sizedict2[v] = sizedict2.get(v, set()) | set([k])\n",
    "\n",
    "print(f\"len1: {len(keys1)}, len2: {len(keys2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size(192): 113 != 76\n",
      "Size(110592): 0 != 12\n",
      "Size(576): 0 != 12\n",
      "Size(36864): 49 != 12\n",
      "len1: 162, len2: 112\n"
     ]
    }
   ],
   "source": [
    "for key in sizedict2:\n",
    "    len1 = 0\n",
    "    len2 = len(sizedict2[key])\n",
    "    if key in sizedict1:\n",
    "        len1 = len(sizedict1[key])\n",
    "    if len1 == len2:\n",
    "        # print(f\"{len1} == {len2}\")\n",
    "        keys1 = keys1 - sizedict1[key]\n",
    "        keys2 = keys2 - sizedict2[key]\n",
    "    else:\n",
    "        print(f\"Size({key}): {len1} != {len2}\")\n",
    "\n",
    "print(f\"len1: {len(keys1)}, len2: {len(keys2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len1: 127, len2: 77\n"
     ]
    }
   ],
   "source": [
    "keys1 = [x for x in list(params1.keys()) if x in params1.keys() if x in keys1]\n",
    "keys2 = [x for x in list(params2.keys()) if x in params2.keys() if x in keys2]\n",
    "\n",
    "i1 = 0\n",
    "i2 = 0\n",
    "while i1 < len(keys1) and i2 < len(keys2):\n",
    "    if params1[keys1[i1]] < params2[keys2[i2]]:\n",
    "        i2 += 1\n",
    "    elif params1[keys1[i1]] > params2[keys2[i2]]:\n",
    "        i1 += 1\n",
    "    elif keys1[i1].split(\".\")[-1] == keys2[i2].split(\".\")[-1]:\n",
    "        print(f\"{keys1[i1]} matches {keys2[i2]}\")\n",
    "        del keys1[i1]\n",
    "        del keys2[i2]\n",
    "    else:\n",
    "        # print(f\"{keys1[i1]} does not match {keys2[i2]}\")\n",
    "        i1 += 1\n",
    "        i2 += 1\n",
    "\n",
    "print(f\"len1: {len(keys1)}, len2: {len(keys2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key prefixes: ['model.encoder', 'model.layernorm', 'model.pooler']\n",
      "Key prefixes: ['model.blocks', 'model.norm']\n"
     ]
    }
   ],
   "source": [
    "depth = 3\n",
    "print(f\"Key prefixes: {sorted(set('.'.join(x.split('.')[:depth]) for x in keys1))}\")\n",
    "print(f\"Key prefixes: {sorted(set('.'.join(x.split('.')[:depth]) for x in keys2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.PosixPath = temp  # revert back to Linux"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
