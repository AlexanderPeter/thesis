{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from IPython.display import display_html\n",
    "# from scipy.special import softmax\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "# from torchvision import transforms\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "from local_python.local_utils import (\n",
    "    load_model,\n",
    "    load_pd_from_json,\n",
    "    print_parameters,\n",
    "    set_seed,\n",
    ")\n",
    "from local_python.dataset_util import (\n",
    "    create_dataloaders,\n",
    ")\n",
    "\n",
    "# from local_python.feature_evaluation import calculate_scores\n",
    "from lora_vit.models import LoRA_ViT_timm\n",
    "from ssl_library.src.models.fine_tuning.classifiers import LinearClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration_csv_path = \"./configs/finetune-configuration.csv\"\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>strategy</th>\n",
       "      <th>dataset_path</th>\n",
       "      <th>checkpoint_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>lora_2</td>\n",
       "      <td>../data_splits/HAM10000_split.csv</td>\n",
       "      <td>../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lora_4</td>\n",
       "      <td>../data_splits/HAM10000_split.csv</td>\n",
       "      <td>../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lora_6</td>\n",
       "      <td>../data_splits/HAM10000_split.csv</td>\n",
       "      <td>../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed strategy                       dataset_path  \\\n",
       "0     1   lora_2  ../data_splits/HAM10000_split.csv   \n",
       "1     1   lora_4  ../data_splits/HAM10000_split.csv   \n",
       "2     1   lora_6  ../data_splits/HAM10000_split.csv   \n",
       "\n",
       "                                     checkpoint_path  \n",
       "0  ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...  \n",
       "1  ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...  \n",
       "2  ../model_weights/vit_t16_v2/ViT_T16-ImageNet_1...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_config = pd.read_csv(configuration_csv_path)\n",
    "df_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(checkpoint_path, strategy, num_classes, image_shape):\n",
    "    strategy_params = strategy.split(\"_\")\n",
    "\n",
    "    if \"concat\" == strategy_params[0]:\n",
    "        rank = int(strategy_params[1])\n",
    "        model = load_model(checkpoint_path, freeze=False, use_ssl_library=True)\n",
    "        params = list(model.parameters())\n",
    "        for param in params[: len(params) - rank]:\n",
    "            param.requires_grad = False\n",
    "        summary = torchinfo.summary(model, image_shape, batch_dim=0)\n",
    "        last_output = summary.summary_list[-1].output_size[-1]\n",
    "        model.head = LinearClassifier(\n",
    "            last_output,\n",
    "            num_labels=num_classes,\n",
    "            use_dropout_in_head=True,\n",
    "            large_head=False,\n",
    "            use_bn=True,\n",
    "        )\n",
    "    elif \"lora\" == strategy_params[0]:\n",
    "        rank = int(strategy_params[1])\n",
    "        model = load_model(checkpoint_path, freeze=True, use_ssl_library=False)\n",
    "        summary = torchinfo.summary(model, image_shape, batch_dim=0)\n",
    "        last_output = summary.summary_list[-1].output_size[-1]\n",
    "        assert hasattr(model, \"blocks\"), f\"Unknown model type: {type(model)}\"\n",
    "        model.head = LinearClassifier(\n",
    "            last_output,\n",
    "            num_labels=num_classes,\n",
    "            use_dropout_in_head=True,\n",
    "            large_head=False,\n",
    "            use_bn=True,\n",
    "        )\n",
    "        model = LoRA_ViT_timm(vit_model=model, r=rank, alpha=4)\n",
    "    else:\n",
    "        assert False, f\"Unknown strategy: {strategy}\"\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_eval(\n",
    "    model, optimizer, criterion, start_epoch, end_epoch, dataloaders, loss_file_path, best_loss = None\n",
    "):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        model.train()\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        with open(loss_file_path, \"a\") as detaillog:\n",
    "            for i, (images, targets) in enumerate(tqdm(dataloaders[\"train\"])):\n",
    "                images = images.to(device)\n",
    "                targets = torch.as_tensor(targets).to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                line = {}\n",
    "                line[\"epoch\"] = epoch\n",
    "                line[\"iteration\"] = i\n",
    "                line[\"loss\"] = loss.item()\n",
    "                line[\"set\"] = \"train\"\n",
    "                json.dump(line, detaillog, indent=2)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(run_path, f\"checkpoint_latest.pth\"),\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        with torch.no_grad():\n",
    "            with open(loss_file_path, \"a\") as detaillog:\n",
    "                for i, (images, targets) in enumerate(tqdm(dataloaders[\"valid\"])):\n",
    "                    images = images.to(device)\n",
    "                    targets = torch.as_tensor(targets).to(device)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    valid_loss.append(loss.item())\n",
    "\n",
    "                    line = {}\n",
    "                    line[\"epoch\"] = epoch\n",
    "                    line[\"iteration\"] = i\n",
    "                    line[\"loss\"] = valid_loss[-1]\n",
    "                    line[\"set\"] = \"valid\"\n",
    "                    json.dump(line, detaillog, indent=2)\n",
    "        \n",
    "        mean_loss = np.array(valid_loss).mean()\n",
    "        if best_loss is None or best_loss < mean_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(run_path, f\"checkpoint_best.pth\"),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 960\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "n_devices = torch.cuda.device_count()\n",
    "for i in range(0, n_devices):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seed to 1\n",
      "Results will be saved to ../runs/HAM10000\\ViT_T16-ImageNet_1k_SSL_Dino\\lora_2_1\n",
      "Set train size: 8908\n",
      "Set valid size: 1103\n",
      "Train class (im)balance: {'bkl': 971, 'nv': 5969, 'df': 103, 'mel': 992, 'vasc': 128, 'bcc': 455, 'akiec': 290}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vit_tiny_patch16_224 from timm-library\n",
      "Ignoring prefix 'model.'\n",
      "Trainable parameters: 18432/5544583\n",
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [12:47<00:00,  5.48s/it]\n",
      "100%|██████████| 18/18 [00:15<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 121/140 [14:14<01:52,  5.90s/it]"
     ]
    }
   ],
   "source": [
    "for _, row in df_config.iterrows():\n",
    "    seed = row[\"seed\"]\n",
    "    set_seed(seed)\n",
    "\n",
    "    checkpoint_path = row[\"checkpoint_path\"]\n",
    "    model_name = os.path.splitext(os.path.basename(checkpoint_path))[0].replace(\n",
    "        \"_headless\", \"\"\n",
    "    )\n",
    "\n",
    "    dataset_path = row[\"dataset_path\"]\n",
    "    dataset_name = os.path.splitext(os.path.basename(dataset_path))[0].replace(\n",
    "        \"_split\", \"\"\n",
    "    )\n",
    "\n",
    "    strategy = row[\"strategy\"]\n",
    "\n",
    "    run_path = os.path.join(\"../runs/\", dataset_name, model_name, f\"{strategy}_{seed}\")\n",
    "    if not os.path.exists(run_path):\n",
    "        os.makedirs(run_path)\n",
    "    print(f\"Results will be saved to {run_path}\")\n",
    "    loss_file_path = os.path.join(run_path, \"loss.txt\")\n",
    "\n",
    "    dataloaders = create_dataloaders(dataset_path, batch_size=batch_size)\n",
    "    train_class_counts = dataloaders[\"train\"].dataset.get_class_counts()\n",
    "    print(f\"Train class (im)balance: {train_class_counts}\")\n",
    "    num_classes = len(train_class_counts)\n",
    "\n",
    "    images, _ = next(iter(dataloaders[\"valid\"]))\n",
    "    image_shape = images.shape[1:]\n",
    "\n",
    "    model = prepare_model(checkpoint_path, strategy, num_classes, image_shape)\n",
    "    print_parameters(model)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        1.0 / np.array(list(train_class_counts.values())), dtype=torch.float\n",
    "    )\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss(weight=class_weights_tensor, reduction=\"mean\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    latest_epoch = -1\n",
    "    best_loss = None\n",
    "    checkpoint_path = os.path.join(run_path, f\"checkpoint_latest.pth\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "        model.load_state_dict(checkpoint, strict=True)\n",
    "        df_metrics = load_pd_from_json(loss_file_path)\n",
    "        latest_epoch = df_metrics[\"epoch\"].max()\n",
    "        print(f\"Latest epoch: {latest_epoch}\")\n",
    "        checkpoint_path = os.path.join(run_path, f\"checkpoint_best.pth\")\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            df_losses = df_metrics[df_metrics[\"set\"] == \"valid\"].groupby([\"epoch\"])[\"loss\"].mean()\n",
    "            best_epoch = df_losses.argmin()\n",
    "            best_loss = df_losses.min()\n",
    "            print(f\"Best epoch: {best_epoch} with {best_loss}\")\n",
    "\n",
    "    start_epoch = latest_epoch + 1\n",
    "    end_epoch = start_epoch + 30\n",
    "\n",
    "    train_eval(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        start_epoch,\n",
    "        end_epoch,\n",
    "        dataloaders,\n",
    "        loss_file_path,\n",
    "        best_loss = best_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchinfo.summary(model, image_shape, batch_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 19\n",
    "\n",
    "# checkpoint_path = \"../model_weights/vit_t16_v2/ViT_T16-ImageNet_1k_SSL_Dino_headless.pth\n",
    "# checkpoint_path = \"../model_weights/vit_t16_v2/ViT_T16-Plant_SSL_Dino_headless.pth\"\n",
    "# checkpoint_path = \"../model_weights/vit_t16_v2/ViT_T16-Derma_SSL_Dino_headless.pth\"\n",
    "\n",
    "# dataset_path = \"../datasets/ddi-diverse-dermatology-images/split.csv\"\n",
    "# dataset_path = \"../datasets/PAD-UFES-20/images/split.csv\"\n",
    "# dataset_path = \"../datasets/HAM10000/images/split.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval(model, data_loader_valid, sample=False, verbose=False):\n",
    "#     model = model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     label_tensor = torch.zeros(0)\n",
    "#     pred_tensor = torch.zeros(0)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in tqdm(data_loader_valid):\n",
    "#             target_tensor = torch.as_tensor([label_map[target] for target in targets])\n",
    "#             label_tensor = torch.cat([label_tensor, target_tensor])\n",
    "\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "#             pred_tensor = torch.cat([pred_tensor, preds.view(-1).cpu()])\n",
    "#             if verbose:\n",
    "#                 for output, target in zip(outputs, targets):\n",
    "#                     probs = softmax(output.cpu().detach().numpy())\n",
    "#                     max_idx = np.argmax(probs)\n",
    "#                     print(\n",
    "#                         f\"Actual class {label_map[target]}, predicted class {max_idx} with probability {probs[max_idx]} \"\n",
    "#                     )\n",
    "\n",
    "#             if sample:\n",
    "#                 break\n",
    "\n",
    "#         # target_tensor = target_tensor.to(device)\n",
    "\n",
    "#     labels = label_tensor.numpy()\n",
    "#     preds = pred_tensor.numpy()\n",
    "\n",
    "#     label_names = [k for k, v in label_map.items() if (v in labels or v in preds)]\n",
    "#     cm = confusion_matrix(labels, preds)\n",
    "#     cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "#     cm_display.plot()\n",
    "#     plt.ylabel(\"True label\")\n",
    "#     plt.xlabel(\"Predicted label\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def load_checkpoint_by_epoch(model, epoch):\n",
    "#     checkpoint = os.path.join(run_path, f\"checkpoint_{epoch}.pth\")\n",
    "#     print(f\"Loading {checkpoint}\")\n",
    "#     checkpoint = torch.load(checkpoint, map_location=torch.device(\"cpu\"))\n",
    "#     model.load_state_dict(checkpoint, strict=True)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def display_side_by_side(dfs, captions=[]):\n",
    "#     html_string = \"\"\n",
    "#     for i, df in enumerate(dfs):\n",
    "#         styler = df.style.set_table_attributes(\"style='display:inline'\")\n",
    "#         if i < len(captions):\n",
    "#             styler.set_caption(captions[i])\n",
    "#         html_string += styler.to_html()\n",
    "#     display_html(html_string, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available_epochs = []\n",
    "# for name in os.listdir(path=run_path):\n",
    "#     match = re.search(\"checkpoint_(\\d+)\\.pth\", name)\n",
    "#     if match:\n",
    "#         available_epochs.append(int(match.group(1)))\n",
    "# latest_epoch = -1\n",
    "# if 0 < len(available_epochs):\n",
    "#     latest_epoch = max(available_epochs)\n",
    "#     print(f\"{len(available_epochs)} checkpoints found. Latest epoch: {latest_epoch}\")\n",
    "#     model = load_checkpoint_by_epoch(model, latest_epoch)\n",
    "# else:\n",
    "#     print(f\"No checkpoint found in {run_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval(model, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metrics = load_pd_from_json(loss_file_path)\n",
    "# metric_columns = [\n",
    "#     column\n",
    "#     for column in df_metrics.columns.values\n",
    "#     if column not in [\"epoch\", \"iteration\", \"set\"]\n",
    "# ]\n",
    "# df_metrics = (\n",
    "#     df_metrics\n",
    "#     .groupby([\"set\", \"epoch\"])[metric_columns]\n",
    "#     .mean()\n",
    "# ).reset_index()\n",
    "\n",
    "# n = 3\n",
    "# for metric_column in metric_columns:\n",
    "#     df_temp = df_metrics[df_metrics[\"set\"] == \"valid\"].sort_values(metric_column)\n",
    "#     display_side_by_side(\n",
    "#         [df_temp.head(n=n), df_temp.tail(n=n)],\n",
    "#         [\n",
    "#             f\"{n} epochs with lowest validation {metric_column}:\",\n",
    "#             f\"{n} epochs with highest validation {metric_column}:\",\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_column = \"loss\"\n",
    "# x_column = \"epoch\"\n",
    "# fig, ax = plt.subplots()\n",
    "# plt.figure()\n",
    "# for set_name in df_metrics[\"set\"].unique():\n",
    "#     df_metrics[df_metrics[\"set\"] == set_name].plot.line(\n",
    "#         y=y_column, x=x_column, label=set_name, ax=ax\n",
    "#     )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_epoch = df_metrics[\"loss\"].idxmin()\n",
    "# print(f\"Best epoch by validation loss is {best_epoch}\")\n",
    "# model = load_checkpoint_by_epoch(model, best_epoch)\n",
    "# eval(model, dl_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "412f64844b7b8af74bdbb265a073648c443bf75ef51a1e949163ab9198702ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
