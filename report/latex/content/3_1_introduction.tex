\chapter{Introduction}\label{chapter_1}
% This chapter explains the basic problem and motivation. 
% Further the research questions and the respective expectation are discussed. 
% It also defines the expected results of this project.

\section{Problem definition}

\gls{ml} models are sucessfully applied in various visual tasks, but many medical fields lack large datasets to develop such models \autocite{castro2019}. 
Ideally, numerous samples per class in different variants are available to ensure, that the model learns to generalize and performs well on unseen data. 
However, regulations such as data protection laws often hinder collecting or publishing medical records. 
While some medical institutions are able to collect at least a small amount of suitable data, they are usually not allowed to publish it publicly. 
In addition to their often rather small size, these small collections are sometimes biased to local characteristics, such as light skin pigmentation in Europea and Asia.
Another obstacle is the obtaining of the according labels. Creating high-quality labels requires experienced specialists. 
The high costs compared to the limited scope may render the project financially not worthwhile.

\subsection{Generic pre-training}
% One approach is to use weights of models pre-trained on generic images like ImageNet dataset and fine-tune the model on a smaller domain specific dataset. 
Wherever possible, it is state-of-the-art to use model weights pre-trained on generic data. 
Common pre-training datasets, such as ImageNet, mostly reduce the training duration and may also improve the resulting accuracy \autocite{deng2009}.
However, their structure often differs greatly from most medical use cases, reducing the effect of pre-training. 

The photos of the ImageNet data set depict various entities like animals or vehicles in a natural environment. 
Therefore, the lightning and background greatly varies even between images with the same class.
Clinical images, on the other hand, are often recorded with a consistent procedure and tool, resulting in less diverse images.
The varying diversity also shows in the number of classes. 
Usually only a single or two-digit number of diseases can be diagnosed from clinical images, while the ImageNet set covers a whole of 1'000 classes.

The image resolution may also differ, but usually renders no big deal, since it can be adjusted quite easily. 
Some model architectures are limited to a specific input size and require the training data to be adapted anyway.
The photos from ImageNet are all standardized to a resolution of $224 \times 224 \times 3$, which can be done for clinical images in the same way.
Different aspect ratios should be considered in this case, though, to prevent side effects from excessive stretching or cropping.

An interesting use case are Medical images, such as X-ray radiographs or \gls{mri} images, since they usually have a single color channels only
In addition, the objects on radiographs have smooth outlines and overlap each other, which opposes the rather clear contours of objects from ImageNet. 
It is also possible to use ImageNet weights for this case by stacking the grayscale layer three times on top of each other.
\autocite{ke2021} trained and compared various \gls{ann} models on the same radiograph task, once from scratch and again using pre-trained ImageNet weights.
Although no general statement can be made, he shows that despite all the differences many models still profit from the generic pre-training.

\subsection{Domain specific pre-training}

Using weights pre-trained on data similar to the target domain often leads to better results than with generic or no pre-training.
This brings us to the initial problem again of not having a sufficient amount of domain specific data.
\gls{ssl} enables the usage of unlabeled data to train basic models, thus saving at least the costs of labeling large quantities of data.

\autocite{cho2023} created such a basic model based on X-ray radiographs and trained a downstream task, which is distinct from the pre-training data.
He also trained the same downstream task with generic pre-trained weights and completely from scratch.
The improvement due to generic pre-training compared to no pre-training is confirmed and an even larger improvement was achieved with his basic model.
He further shows, that with his basic model a fraction of the training data can be omitted and the same results can still be achieved as without the basic model, but the full amount of data for downstream task. 
% The basic model is publicly available and can be used for 

\subsection{Cross-domain pre-training}
It is not yet clear how to determine in advance if a domain is suited for pre-training a model. 
It is also not clear how to separate suitable and unsuitable images within a data set to make the best of the pre-training.
% It is suspected, that range and stuff

However, cross-domain pre-training has been successfull applied between different radiograph datasets \autocite{cohen2020} and different skin lesion datasets \autocite{krammer2022}.

Due to the sparsity of all clinical images, using clinical images from another related data set is still limiting.
Plant disease images resemble to skin disease images in many aspects. 
In both cases the diseases usually manifest in distinctive discolorations. %which can be used to classify the cause and enables according treatment.
The spot of an infected leaf has a similar characteristic as a mole on human skin.
Plant disease datasets are, in contrary to dermatology images, publicly available in large quantities and are therefore suited to try out this approach.

\section{Basic goal and expected results}
This work aims to evaluate \gls{ml} models with different pre-training approaches on plant disease and dermatology diagnostic tasks.
The different strategies and the respective performance ranking is listed in Table \ref{tab:expected_results}.

\begin{table}[H]
    \centering
    \caption{Expected rankings \label{tab:expected_results}}
    \begin{tabularx}{\textwidth}{|
        >{\hsize=.4\hsize}X |
        >{\hsize=.3\hsize}X |
        >{\hsize=.3\hsize}X |
    }
    \hline
    \textbf{Pre-training data} & \textbf{Plant tasks} & \textbf{Dermatology tasks} \tabularnewline \hline
    No pre-training & 4. & 4. \tabularnewline \hline
    Generic data (\gls{sl}/\gls{ssl}) & 3. & 3. \tabularnewline \hline
    Plant diseases (\gls{sl}/\gls{ssl}) & 1. & 2. \tabularnewline \hline
    Dermatology (\gls{ssl}) & 2. & 1. \tabularnewline \hline
    \end{tabularx} 
\end{table}

The domain specific pre-training is expected to achieve the best results.
ideally, cross-domain pre-training will surpass the generic pre-training and have the second best result.
Transfer learning probably will work not only from plant disease to dermatology diagnostics, but the other way around as well.


Furthermore, should be measured how limiting downstream images impacts the resulting performance. 
The results of conventional approaches and the results from pre-training on plant diseases and limited clinical data can then be compared to estimate the savings potential.
These main questions are also listed in the task description in the appendix~\ref{appendix:task_description}.

\section{Notes on this report}
\subsection{Basic organization}
The organization of this report is mainly based on the structure given by \gls{hslu} during the bachelor's program \autocite{balzert2011}.
Chapter \ref{chapter_1} explains the problem with existing approaches and lists the research questions with the expectated results. 
Furthermore organizational notes are provided to understand the structure and notation of the report.
Chapter \ref{chapter_2} first introduces the main \gls{ml} concepts to allow less versed readers to follow through the report.
It also provides some relevant findings from the project specific literature research.
Chapter \ref{chapter_3} covers the experimental setup including the selected datasets and their main characteristics.
Chapter \ref{chapter_4} presents the received results in comparison with each other.
Chapter \ref{chapter_5} discusses the main findings of this work and proposes areas for further research. 

\subsection{Notation}

The following Table~\ref{tab:notation} shows the notations used in this work, which are not self-explanatory.

\begin{table}[H]
\centering
\caption{Notations in this work.\label{tab:notation}}
\begin{tabularx}{\textwidth}{|
 >{\hsize=.2\hsize}X |
 >{\hsize=.8\hsize}X |
}
\hline
\textbf{Math symbol} & \textbf{Description} \\ \hline
$\times$ & Dimension sign \\ \hline
$\cdot$ & Matrix product \\ \hline
$\partial$ & Partial derivative \\ \hline
$\nabla$ & Gradient \\ \hline
$\ast$ & Convolution \\ \hline
$:=$ & Defined as being equal to \\ \hline
$a$ & Scalar value \\ \hline 
$\mathbf{a}$ & Row or column vector \\ \hline
$\mathbf{a}^{[i]}$ & Vector with index $i$  \\ \hline
$\mathbf{A}$ & Matrix \\ \hline 
$\mathbf{A}^{[i]}$ & Matrix with index $i$ \\ \hline 
$\mathbf{X}^{-1}$ & Inverse matrix of $\mathbf{X}$ \\ \hline 
$\mathbf{X}^{T}$ & Transformation matrix of $\mathbf{X}$ \\ \hline 
$a_i$ & Entry of vector $\mathbf{a}$ with index $i$ \\ \hline 
$a_{i,j}$ & Entry of matrix $\mathbf{A}$ in row $i$ and column $j$ \\ \hline
$\hat{y}$ & Estimation for $y$ \\ \hline 
$\mathbb{R}$ & The set of real numbers \\ \hline
$\mathcal{L}$ & Loss function \\ \hline
$\mathcal{O}$ & Big O notation \\ \hline
\end{tabularx}
\end{table}

% $\ldots$ & Placeholder horizontal \\ \hline
% $\ddots$ & Placeholder diagonal \\ \hline
% $\vdots$ & Placeholder vertical \\ \hline
